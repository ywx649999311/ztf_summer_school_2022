{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Monte Carlo\n",
    "\n",
    "A posterior is already naturally factored into a likelihood function and a prior PDF.\n",
    "\n",
    "## $$p(\\theta|x) \\propto p(x|\\theta)\\,p(\\theta)$$\n",
    "\n",
    "Applying this in the MC integration context leads to the Simple Monte Carlo algorithm:\n",
    "\n",
    "```\n",
    "while we want more samples\n",
    "    draw theta from p(theta)\n",
    "    compute weight = p(x|theta)\n",
    "    store theta and weight\n",
    "```\n",
    "\n",
    "Obtaining marginal distribution(s) for $\\theta$ then reduces to constructing weighted histograms of the samples.\n",
    "\n",
    "SMC is indeed simple (as long as the prior is simple to draw from), but if the priors are not very informative then it still wastes many likelihood evaluations where the posterior is small. \n",
    "\n",
    "However, refinements of this approach lead to some more advanced algorithms. Once class of refinements is in how the samples are drawn:\n",
    "\n",
    "### Rejection sampling\n",
    "For this method, we need to define an *envelope function* which everywhere exceeds the target PDF, $p(x)$, and can be sampled. Let this be $Ag(x)$ where $A$ is a scaling factor and $g(x)$ is a PDF we know.\n",
    "\n",
    "Then the algorithm is\n",
    "```\n",
    "while we want more samples\n",
    "    draw a random value for x from some distribution g in the variable x\n",
    "    draw u from Uniform(0,1)\n",
    "    if u <= p(x)/(A*g(x)), keep the sample x\n",
    "    otherwise, reject x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Rejection Sampling\n",
    "\n",
    "Implement a rejection sampler corresponding to the example figure above that illustrates the method. For this example,\n",
    "\n",
    "* $p(x)$ is the $\\chi^2$ distribution with 3 degrees of freedom\n",
    "\n",
    "* $A=\\pi$\n",
    "\n",
    "* $g(x)$ is a normal distribution with mean 0 and standard deviation 5\n",
    "\n",
    "Verify that your samples do indeed approximate the target PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is at the heart of a  very widely used (by scientists anyway) and simple to implement technique: the **Metropolis-Hastings** algorithm, which is one example of *Markov Chain Monte Carlo.*\n",
    "\n",
    "### Markov Chains\n",
    "\n",
    "A Markov Chain is a sequence where the $n$th entry depends explicitly on the $(n-1)$th, but not (explicitly) on previous steps. The chain will be a random walk through parameter space.\n",
    "\n",
    "\n",
    "### Formalities of MCMC\n",
    "\n",
    "Markov chains provide a powerful way to sample PDFs, provided that the transition kernel/proposal distribution - how we go from state 1 to state 2  satisfies a few requirements\n",
    "* Detailed balance: any transition must be reversible; the probability of being at $x$ and moving to $y$ must equal the probability of being at $y$ and moving to $x$\n",
    "* Ergodicity: the process may not be periodic, but it nevertheless must be possible to return to a given state in finite time\n",
    "* It must be possible, in principle, to reach any state with non-zero prior probability\n",
    "\n",
    "#### Why does it work?\n",
    "The probability of an arbitrary point from such a chain being located at $x'$ is (marginalizing over the possible immediately preceding points)\n",
    "\n",
    "## $$p(x') = \\int dx \\, p(x) \\, T(x'|x)$$\n",
    "\n",
    "where $T(x'|x)$ is the transition probability of a step from $x$ to $x'$.\n",
    "\n",
    "If we have detailed balance, \n",
    "\n",
    "## $$p(x)T(x'|x) = p(x')T(x|x')$$\n",
    "\n",
    "rearranging:\n",
    "\n",
    "## $$ \\frac{T(x'|x)}{T(x|x')} = \\frac{p(x')}{p(x)} $$\n",
    "\n",
    "The basic trick to connect this with rejection sampling is to break the transition into two steps:\n",
    "1. A proposal, g(x'| x)\n",
    "and \n",
    "2. Acceptance ratio, A(x'|x)\n",
    "\n",
    "i.e. \n",
    "\n",
    "## $$ T(x'|x) = A(x'|x) g(x'| x) $$ \n",
    "\n",
    "rearranging again :\n",
    "\n",
    "## $$ \\frac{A(x'|x)}{A(x|x')} = \\frac{p(x')g(x|x')}{p(x)g(x'|x) }$$\n",
    "\n",
    "\n",
    "## Metropolis-Hastings\n",
    "This algorithm can be thought of as an MCMC adaptation of rejection sampling. We need to define\n",
    "1. An initial state (parameter values)\n",
    "2. A proposal distribution, $g(x'|x)$, giving the probability that we attempt to move from $x$ to $x'$.\n",
    "\n",
    "Let $P$ be the distribution we want to sample. The algorithm is then\n",
    "```\n",
    "set x to an initial state\n",
    "compute p(x)\n",
    "while we want more samples\n",
    "    draw x' from the proposal distribution g(x'|x)\n",
    "    compute p(x')\n",
    "    draw u from Uniform(0,1)\n",
    "    if u <= p(x')/p(x) * g(x|x')/g(x'|x), set the state x=x'\n",
    "    otherwise, x stays the same\n",
    "    store x as a sample\n",
    "```\n",
    "\n",
    "Compare this to the rejection sampling algorithm above!\n",
    "\n",
    "Notice that the probability of accepting a step  (once it's proposed) is\n",
    "\n",
    "## $$A(x',x) = \\mathrm{min}\\left[1, \\frac{p(x')g(x|x')}{p(x)g(x'|x)}\\right]$$\n",
    "\n",
    "Let's look again at the requirement of detailed balance\n",
    "\n",
    "> the probability of being at $x$ and moving to $y$ must equal the probability of being at $y$ and moving to $x$\n",
    "\n",
    "The first of these is $p(x)g(x'|x)A(x',x)$, where\n",
    "\n",
    "* $p(x)$ is the posterior density (probability of *being* at $x$, if we're sampling $P$ properly)\n",
    "\n",
    "* $g(x'|x)$ is the proposal distribution (probability of attempting a move to $x'$ from $x$)\n",
    "\n",
    "* $A(x',x)$ is the probability of accepting the proposed move\n",
    "\n",
    "With this definition of $A$, detailed balance is automatically satisfied!\n",
    "\n",
    "## $$p(x)g(x'|x)A(x',x) \\equiv p(x')g(x|x')A(x,x')$$\n",
    "\n",
    "Note that **even if a step is rejected, we still keep a sample** (the original state, without moving). The difficulty of finding a temptingly better point is important information!\n",
    "\n",
    "\n",
    "### Metropolis\n",
    "If the proposal distribution is translation invariant (i.e. only depends on the distance between the points), $g(x'|x)=g\\left(\\left|x'-x\\right|\\right)$, then it drops out of the *acceptance ratio* that decides whether to accept a step. \n",
    "\n",
    "**The most basic choice you can make is a Gaussian.**\n",
    "\n",
    "For an N-dimensional Gaussian proposal function, $g$,  an *acceptance fraction* $A$ of $\\sim25\\%$ is optimal.\n",
    "\n",
    "## $$A(x',x) = \\mathrm{min}\\left[1, \\frac{p(x')}{p(x)}\\right]$$\n",
    "\n",
    "This is the original Metropolis algorithm, and is the easiest case to implement.\n",
    "\n",
    "In this case, we *always* accept a jump to higher $p$, and *sometimes* accept one to lower $p$.\n",
    "\n",
    "You'll have to tune the $\\sigma$ of your Gaussian proposal function g by hand to make sure that your get a ~25% acceptance ratio. Note that this sigma is simply telling you the distribution of $x'$ from $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Metropolis\n",
    "\n",
    "\n",
    "You guessed it... Implement the Metroplis Hastings algorithm in python.\n",
    "Your implementation should accept an arbitary function as an argument (just as you've been passing to `scipy.optimize`)\n",
    "\n",
    "Test it by sampling both the `circle` and `pgauss` function below 10,000 times and plotting your samples as you did in class (don't plot up your rejected). You can implement a prior function that imposes bounds. \n",
    "\n",
    "Look at your first 100 samples and then samples 500 to 600 for the `pgauss` case and comment on the difference. Think about this on Problem 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "def circle(x, y):\n",
    "    return (x-1)**2 + (y-2)**2 - 3**2\n",
    "\n",
    "mus = np.array([5, 5])\n",
    "sigmas = np.array([[1, .9], [.9, 1]])\n",
    "\n",
    "def pgauss(x, y):\n",
    "    return st.multivariate_normal.pdf([x, y], mean=mus, cov=sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next HW: Using your sampler for a real problem where a grid would have been really painful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
